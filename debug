import os
import re
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
CHUNK_SIZE    = 2000                # number of records per chunk
INPUT_FILE    = "input.txt"
EMAIL_REGEX   = r"\b[A-Za-z0-9._%+-]+@(?:[A-Za-z0-9-]+\.)+[A-Za-z]{2,}\b"
OUTPUT_DOCX   = "output_email.docx"
OUTPUT_XLSX   = "output_email.xlsx"
TEMP_DIR      = "temp_parts"

# === SPLIT FILE INTO RECORD-BASED CHUNKS ===
def split_file():
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)

    # read all lines, normalize Windows endings
    with open(INPUT_FILE, 'r', encoding='utf-8', errors='ignore') as f:
        raw = f.readlines()
    lines = [ln.rstrip('\r\n') + '\n' for ln in raw]

    # group into record-pairs
    records = []
    for i in range(0, len(lines), 2):
        records.append(lines[i])
        records.append(lines[i+1] if i+1 < len(lines) else "\n")

    # write out chunks, CHUNK_SIZE records => 2*CHUNK_SIZE lines per chunk
    recs_per_chunk = CHUNK_SIZE * 2
    for idx in range(0, len(records), recs_per_chunk):
        part = idx // recs_per_chunk
        chunk = records[idx: idx + recs_per_chunk]
        with open(os.path.join(TEMP_DIR, f"chunk_{part}.txt"), 'w', encoding='utf-8') as out:
            out.writelines(chunk)

# === PROCESS ONE CHUNK ===
def process_chunk(args):
    chunk_path, result_list = args
    pattern = re.compile(EMAIL_REGEX, re.IGNORECASE)

    doc = Document()
    matches_data = []
    total_recs = matched_recs = 0

    with open(chunk_path, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()

    # each record is two lines: identifier + payload
    for i in range(0, len(lines), 2):
        total_recs += 1
        identifier = lines[i].rstrip('\r\n')
        payload_line = lines[i+1].rstrip('\r\n') if i+1 < len(lines) else ""
        m = re.match(r"^CreateTime:(\d+)\s+(.*)$", payload_line)
        if not m:
            continue
        timestamp    = m.group(1)
        payload_text = m.group(2)

        # find email matches in the payload only
        matches = list(pattern.finditer(payload_text))
        if not matches:
            continue
        matched_recs += 1

        # build Word paragraph: identifier | timestamp | payload with email in red
        para = doc.add_paragraph(f"{identifier} | CreateTime:{timestamp} | ")
        last = 0
        for mt in matches:
            s, e = mt.span()
            if s > last:
                para.add_run(payload_text[last:s])
            run = para.add_run(payload_text[s:e])
            run.font.color.rgb = RGBColor(255, 0, 0)
            last = e
            # record for Excel
            matches_data.append((identifier, timestamp, payload_text, mt.group()))
        if last < len(payload_text):
            para.add_run(payload_text[last:])

        # save only first paragraph per record (multiple emails yield multiple Excel rows)
        # Word will show all matches in one paragraph

    # save chunk docx only if matches found
    if matches_data:
        part_id = os.path.splitext(os.path.basename(chunk_path))[0].split('_')[-1]
        doc.save(os.path.join(TEMP_DIR, f"chunk_{part_id}.docx"))

    result_list.append((matches_data, total_recs, matched_recs))

# === MERGE WORD CHUNKS (preserve red runs) ===
def merge_word():
    merged = Document()
    files = sorted(f for f in os.listdir(TEMP_DIR) if f.endswith(".docx"))
    with tqdm(total=len(files), desc="Merging Word files") as pbar:
        for fn in files:
            doc = Document(os.path.join(TEMP_DIR, fn))
            for para in doc.paragraphs:
                new_p = merged.add_paragraph()
                for run in para.runs:
                    nr = new_p.add_run(run.text)
                    if run.font.color and run.font.color.rgb:
                        nr.font.color.rgb = run.font.color.rgb
                    nr.bold      = run.bold
                    nr.italic    = run.italic
                    nr.underline = run.underline
            pbar.update(1)
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL ===
def write_excel(all_matches):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet()
    red = wb.add_format({'font_color': 'red'})

    # header row
    ws.write_row(0, 0, ["Identifier","Timestamp","Payload","Email"])
    row = 1
    for identifier, timestamp, payload, email in all_matches:
        ws.write(row, 0, identifier)
        ws.write(row, 1, timestamp)
        ws.write(row, 2, payload)
        ws.write(row, 3, email, red)
        row += 1

    wb.close()

# === MAIN ===
if __name__ == "__main__":
    print("=== EMAIL EXTRACTOR (Record-Based, Parallel) ===")
    print("Regex used:", EMAIL_REGEX)

    split_file()

    print("\nProcessing chunks in parallel...")
    manager = Manager()
    results = manager.list()

    chunks = [os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR) if f.endswith(".txt")]
    with tqdm(total=len(chunks), desc="Chunks") as pbar, \
         Pool(min(cpu_count(), len(chunks))) as pool:
        for _ in pool.imap_unordered(process_chunk, [(c, results) for c in chunks]):
            pbar.update(1)

    # aggregate
    all_matches       = []
    total_records     = total_with_emails = 0
    for matches_data, recs, matched in results:
        all_matches.extend(matches_data)
        total_records     += recs
        total_with_emails += matched

    print(f"\nRecords scanned       : {total_records}")
    print(f"Records with emails   : {total_with_emails}")
    print(f"Total email matches   : {len(all_matches)}")

    merge_word()
    write_excel(all_matches)

    print("\nCleaning up...")
    shutil.rmtree(TEMP_DIR)

    print(f"\n→ Word saved to {OUTPUT_DOCX}")
    print(f"→ Excel saved to {OUTPUT_XLSX}")
