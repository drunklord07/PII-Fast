import os
import re
import json
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE    = "input.txt"
CHUNK_SIZE    = 2000                    # number of records per parallel chunk
EMAIL_REGEX   = r"\b[A-Za-z0-9._%+-]+@(?:[A-Za-z0-9-]+\.)+[A-Za-z]{2,}\b"
OUTPUT_DOCX   = "output_email.docx"
OUTPUT_XLSX   = "output_email.xlsx"
TEMP_DIR      = "temp_parts"
DEBUG_LOG     = "debug_no_email.log"

# === RECORD PARSING ===
def load_records():
    """
    Reads INPUT_FILE, normalizes line endings, and returns a list of tuples:
      (identifier, timestamp, payload_text)
    by detecting every line that starts with 'CreateTime:' and pairing it with
    the most recent preceding non-empty line as its identifier.
    """
    records = []
    last_id = None
    ct_re   = re.compile(r"^CreateTime:(\d+)\s+(.*)$")
    with open(INPUT_FILE, 'r', encoding='utf-8', errors='ignore') as f:
        for raw in f:
            line = raw.rstrip('\r\n')
            m = ct_re.match(line)
            if m and last_id is not None:
                ts      = m.group(1)
                payload = m.group(2)
                records.append((last_id, ts, payload))
            elif line.strip():
                last_id = line.strip()
    return records

# === CHUNK RECORD LISTS ===
def chunk_records(records):
    for i in range(0, len(records), CHUNK_SIZE):
        yield records[i:i+CHUNK_SIZE], i // CHUNK_SIZE

# === PROCESS ONE CHUNK ===
def process_chunk(args):
    chunk, idx, result_list = args
    pattern = re.compile(EMAIL_REGEX, re.IGNORECASE)

    # Word doc for this chunk
    doc = Document()
    matches_data = []
    recs_seen = recs_with_email = 0

    # open debug log for this worker
    debug_f = open(DEBUG_LOG, 'a', encoding='utf-8')

    for identifier, ts, payload in chunk:
        recs_seen += 1
        matches = list(pattern.finditer(payload))
        if not matches:
            # log this payload for manual inspection
            debug_f.write(f"NO EMAIL in record: {identifier} | CreateTime:{ts} | {payload}\n")
            continue
        recs_with_email += 1

        # parse JSON once for field lookup
        try:
            obj = json.loads(payload)
        except json.JSONDecodeError:
            obj = {}

        # Word paragraph
        para = doc.add_paragraph(f"{identifier} | CreateTime:{ts} | ")
        last = 0
        for mt in matches:
            s, e = mt.span()
            if s > last:
                para.add_run(payload[last:s])
            run = para.add_run(payload[s:e])
            run.font.color.rgb = RGBColor(255, 0, 0)
            last = e
            # collect for Excel
            matches_data.append((identifier, ts, payload, mt.group()))
        if last < len(payload):
            para.add_run(payload[last:])

    debug_f.close()

    # save Word chunk if any matches
    if matches_data:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((matches_data, recs_seen, recs_with_email))

# === MERGE ALL WORD CHUNKS ===
def merge_word():
    merged = Document()
    part_files = sorted(f for f in os.listdir(TEMP_DIR) if f.endswith(".docx"))
    for fn in tqdm(part_files, desc="Merging Word"):
        doc = Document(os.path.join(TEMP_DIR, fn))
        for para in doc.paragraphs:
            new_p = merged.add_paragraph()
            for run in para.runs:
                nr = new_p.add_run(run.text)
                if run.font.color and run.font.color.rgb:
                    nr.font.color.rgb = run.font.color.rgb
                nr.bold = run.bold
                nr.italic = run.italic
                nr.underline = run.underline
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL ===
def write_excel(all_matches):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet()
    red = wb.add_format({'font_color':'red'})

    # header
    ws.write_row(0, 0, ["Identifier","Timestamp","Payload","Email"])
    row = 1
    for identifier, ts, payload, email in all_matches:
        ws.write(row, 0, identifier)
        ws.write(row, 1, ts)
        ws.write(row, 2, payload)
        ws.write(row, 3, email, red)
        row += 1

    wb.close()

# === MAIN ===
if __name__ == "__main__":
    # cleanup old debug log and temp
    if os.path.exists(DEBUG_LOG):
        os.remove(DEBUG_LOG)
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    print("=== EMAIL EXTRACTOR (Robust Record-Based) ===")
    print("Regex:", EMAIL_REGEX)

    # load and chunk
    records = load_records()
    print(f"Total records found: {len(records)}")
    chunks  = list(chunk_records(records))

    manager = Manager()
    results = manager.list()

    # process in parallel
    with Pool(min(cpu_count(), len(chunks))) as pool:
        list(tqdm(pool.imap_unordered(process_chunk,
                                      [ (chunk, idx, results)
                                        for chunk, idx in chunks ]),
                  total=len(chunks),
                  desc="Processing chunks"))

    # aggregate results
    all_matches        = []
    total_recs         = total_with_email = 0
    for matches_data, recs, with_email in results:
        all_matches.extend(matches_data)
        total_recs       += recs
        total_with_email += with_email

    print(f"\nRecords scanned     : {total_recs}")
    print(f"Records with email  : {total_with_email}")
    print(f"Total email matches : {len(all_matches)}")

    # outputs
    if os.path.isdir(TEMP_DIR):
        merge_word()
    write_excel(all_matches)

    # cleanup
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    print(f"\n→ Word saved to {OUTPUT_DOCX}")
    print(f"→ Excel saved to {OUTPUT_XLSX}")
    print(f"→ Debug log at {DEBUG_LOG} for any no-email records.")
