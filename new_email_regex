import os
import re
import json
import shutil
from multiprocessing import Pool, cpu_count, Manager
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
CHUNK_SIZE    = 2000
INPUT_FILE    = "input.txt"
EMAIL_REGEX   = r"(?<![\w@.-])([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[A-Za-z]{2,})(?![\w@.-])"
OUTPUT_DOCX   = "output_email.docx"
OUTPUT_XLSX   = "output_email.xlsx"
TEMP_DIR      = "temp_parts"

# === UTILITIES ===
def flatten_json(y):
    out = {}
    def _flat(x, name=""):
        if isinstance(x, dict):
            for k, v in x.items():
                _flat(v, f"{name}{k}.")
        else:
            out[name[:-1]] = x
    _flat(y)
    return out

# === SPLIT INTO CHUNKS ===
def split_file():
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)

    with open(INPUT_FILE, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()

    total_recs = (len(lines) + 1) // 2
    pbar = tqdm(total=total_recs, desc="Splitting file", unit="records")
    records = []
    for i in range(0, len(lines), 2):
        records.append(lines[i])
        records.append(lines[i+1] if i+1 < len(lines) else "\n")
        pbar.update(1)
    pbar.close()

    recs_per_file = CHUNK_SIZE * 2
    for idx in range(0, len(records), recs_per_file):
        part = idx // recs_per_file
        with open(f"{TEMP_DIR}/chunk_{part}.txt", 'w', encoding='utf-8') as out:
            out.writelines(records[idx:idx + recs_per_file])

# === PROCESS A CHUNK ===
def process_chunk(args):
    chunk_path, result_list = args
    pattern = re.compile(EMAIL_REGEX)
    doc = Document()
    matches_data = []
    total_recs = matched_recs = 0

    with open(chunk_path, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()

    for i in range(0, len(lines), 2):
        total_recs += 1
        topic = lines[i].strip()
        ct_line = lines[i+1].strip() if i+1 < len(lines) else ""
        m = re.match(r'^CreateTime:(\d+)\s+(.*)$', ct_line)
        if not m:
            continue
        timestamp, payload_text = m.group(1), m.group(2)

        try:
            flat = flatten_json(json.loads(payload_text))
        except json.JSONDecodeError:
            flat = {}

        matches = list(pattern.finditer(payload_text))
        if not matches:
            continue
        matched_recs += 1

        # Word paragraph
        para = doc.add_paragraph(f"{topic} | CreateTime:{timestamp} | ")
        last = 0
        for mt in matches:
            s, e = mt.span(1)
            if s > last:
                para.add_run(payload_text[last:s])
            run = para.add_run(payload_text[s:e])
            run.font.color.rgb = RGBColor(255, 0, 0)
            last = e
        if last < len(payload_text):
            para.add_run(payload_text[last:])

        # append field names
        fields = []
        for mt in matches:
            val = mt.group(1)
            field = next((k for k,v in flat.items() if str(v)==val), "")
            if field:
                fields.append(field)
        if fields:
            para.add_run(" | field: ")
            for idx, fld in enumerate(fields):
                if idx: para.add_run(", ")
                run = para.add_run(fld)
                run.font.color.rgb = RGBColor(255, 0, 0)

        for mt in matches:
            val = mt.group(1)
            field = next((k for k,v in flat.items() if str(v)==val), "")
            matches_data.append((topic, timestamp, payload_text, val, field))

    part_id = os.path.splitext(os.path.basename(chunk_path))[0].split('_')[-1]
    doc.save(f"{TEMP_DIR}/chunk_{part_id}.docx")
    result_list.append((matches_data, total_recs, matched_recs))

# === MERGE WORD FILES (PRESERVE RGBColor DIRECTLY) ===
def merge_word():
    merged = Document()
    part_files = sorted(f for f in os.listdir(TEMP_DIR) if f.endswith(".docx"))
    with tqdm(total=len(part_files), desc="Merging Word files") as pbar:
        for fn in part_files:
            doc = Document(os.path.join(TEMP_DIR, fn))
            for para in doc.paragraphs:
                new_para = merged.add_paragraph()
                for run in para.runs:
                    new_run = new_para.add_run(run.text)
                    if run.font.color and run.font.color.rgb:
                        new_run.font.color.rgb = run.font.color.rgb
                    new_run.bold      = run.bold
                    new_run.italic    = run.italic
                    new_run.underline = run.underline
            pbar.update(1)
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL ===
def write_excel(all_matches):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet()
    red = wb.add_format({'font_color':'red'})

    headers = ["Topic","Timestamp","Payload","Match","Field"]
    ws.write_row(0, 0, headers)

    row = 1
    for topic, ts, payload, match, field in all_matches:
        ws.write(row, 0, topic)
        ws.write(row, 1, ts)
        ws.write(row, 2, payload)
        ws.write(row, 3, match, red)
        ws.write(row, 4, field)
        row += 1

    wb.close()

# === MAIN ===
if __name__ == "__main__":
    print("=== EMAIL EXTRACTOR ===")
    print("Regex:", EMAIL_REGEX)

    split_file()
    manager = Manager()
    results = manager.list()

    chunks = [os.path.join(TEMP_DIR,f) for f in os.listdir(TEMP_DIR) if f.endswith(".txt")]
    with tqdm(total=len(chunks), desc="Processing chunks") as pbar, \
         Pool(min(cpu_count(), len(chunks))) as pool:
        for _ in pool.imap_unordered(process_chunk, [(c, results) for c in chunks]):
            pbar.update(1)

    all_matches = []
    scanned = matched = 0
    for data, tot, mt in results:
        all_matches.extend(data)
        scanned += tot
        matched += mt

    print(f"Scanned records: {scanned}, Matched records: {matched}, Total matches: {len(all_matches)}")

    merge_word()
    write_excel(all_matches)

    shutil.rmtree(TEMP_DIR)
    print(f"→ Word saved to {OUTPUT_DOCX}")
    print(f"→ Excel saved to {OUTPUT_XLSX}")
