import os
import re
import json
import shutil
from multiprocessing import Pool, cpu_count, Manager
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
CHUNK_SIZE    = 2000
INPUT_FILE    = "input.txt"
EMAIL_REGEX   = r"(?<![\w@.-])([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[A-Za-z]{2,})(?![\w@.-])"
OUTPUT_DOCX   = "output_email.docx"
OUTPUT_XLSX   = "output_email.xlsx"
TEMP_DIR      = "temp_parts"

# === FIND_FIELD ===
def find_field(obj, val):
    """
    Recursively search obj (dict or list) for a leaf whose string(value) == val.
    Returns list of keys forming the path, or None if not found.
    """
    if isinstance(obj, dict):
        for k, v in obj.items():
            if isinstance(v, list):
                # if it's a list, check primitives first
                for item in v:
                    if not isinstance(item, (dict, list)) and str(item) == val:
                        return [k]
                    # or dive deeper
                    sub = find_field(item, val)
                    if sub:
                        return [k] + sub
            elif isinstance(v, dict):
                sub = find_field(v, val)
                if sub:
                    return [k] + sub
            else:
                if str(v) == val:
                    return [k]
    elif isinstance(obj, list):
        for item in obj:
            sub = find_field(item, val)
            if sub:
                return sub
    return None

# === SPLIT INPUT INTO CHUNKS ===
def split_file():
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)

    lines = open(INPUT_FILE, 'r', encoding='utf-8', errors='ignore').readlines()
    total_recs = (len(lines) + 1) // 2
    pbar = tqdm(total=total_recs, desc="Splitting file", unit="records")
    records = []
    for i in range(0, len(lines), 2):
        records.append(lines[i])
        records.append(lines[i+1] if i+1 < len(lines) else "\n")
        pbar.update(1)
    pbar.close()

    recs_per_file = CHUNK_SIZE * 2
    for idx in range(0, len(records), recs_per_file):
        part = idx // recs_per_file
        with open(f"{TEMP_DIR}/chunk_{part}.txt", 'w', encoding='utf-8') as out:
            out.writelines(records[idx:idx + recs_per_file])

# === PROCESS A CHUNK ===
def process_chunk(args):
    chunk_path, result_list = args
    email_pat = re.compile(EMAIL_REGEX)
    doc = Document()
    matches_data = []
    total_recs = matched_recs = 0

    lines = open(chunk_path, 'r', encoding='utf-8', errors='ignore').readlines()
    for i in range(0, len(lines), 2):
        total_recs += 1
        topic = lines[i].strip()
        ct_line = lines[i+1].strip() if i+1 < len(lines) else ""
        m = re.match(r'^CreateTime:(\d+)\s+(.*)$', ct_line)
        if not m:
            continue
        timestamp, payload_text = m.group(1), m.group(2)

        try:
            payload_obj = json.loads(payload_text)
        except json.JSONDecodeError:
            payload_obj = {}

        # find all email matches
        matches = list(email_pat.finditer(payload_text))
        if not matches:
            continue
        matched_recs += 1

        # Build Word paragraph with both the match and the field highlighted
        para = doc.add_paragraph(f"{topic} | CreateTime:{timestamp} | ")
        last = 0
        fields = []
        for mt in matches:
            s, e = mt.span(1)
            if s > last:
                para.add_run(payload_text[last:s])
            run = para.add_run(payload_text[s:e])
            run.font.color.rgb = RGBColor(255, 0, 0)
            last = e

            # determine field path for this match
            val = mt.group(1)
            path = find_field(payload_obj, val)
            field_name = ".".join(path) if path else ""
            fields.append(field_name)

        if last < len(payload_text):
            para.add_run(payload_text[last:])

        # append all fields in red
        if fields:
            para.add_run(" | field: ")
            for idx, fld in enumerate(fields):
                if idx:
                    para.add_run(", ")
                fld_run = para.add_run(fld)
                fld_run.font.color.rgb = RGBColor(255, 0, 0)

        # Collect for Excel
        for mt, fld in zip(matches, fields):
            matches_data.append((
                topic,
                timestamp,
                payload_text,
                mt.group(1),
                fld
            ))

    # save chunk docx
    part_id = os.path.splitext(os.path.basename(chunk_path))[0].split('_')[-1]
    doc.save(f"{TEMP_DIR}/chunk_{part_id}.docx")
    result_list.append((matches_data, total_recs, matched_recs))

# === MERGE WORD CHUNKS ===
def merge_word():
    merged = Document()
    part_files = sorted(f for f in os.listdir(TEMP_DIR) if f.endswith(".docx"))
    with tqdm(total=len(part_files), desc="Merging Word files") as pbar:
        for fn in part_files:
            doc = Document(os.path.join(TEMP_DIR, fn))
            for para in doc.paragraphs:
                new_para = merged.add_paragraph()
                for run in para.runs:
                    new_run = new_para.add_run(run.text)
                    # copy color directly
                    if run.font.color and run.font.color.rgb:
                        new_run.font.color.rgb = run.font.color.rgb
                    new_run.bold      = run.bold
                    new_run.italic    = run.italic
                    new_run.underline = run.underline
            pbar.update(1)
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL ===
def write_excel(all_matches):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet()
    red = wb.add_format({'font_color': 'red'})

    ws.write_row(0, 0, ["Topic","Timestamp","Payload","Match","Field"])
    row = 1
    for topic, ts, payload, match, field in all_matches:
        ws.write(row, 0, topic)
        ws.write(row, 1, ts)
        ws.write(row, 2, payload)
        ws.write(row, 3, match, red)
        ws.write(row, 4, field)
        row += 1

    wb.close()

# === MAIN ===
if __name__ == "__main__":
    print("=== EMAIL EXTRACTOR (Parallel + Colored Output) ===")
    print("Regex used:", EMAIL_REGEX)

    split_file()
    manager = Manager()
    results = manager.list()

    chunk_files = [os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR) if f.endswith(".txt")]
    with tqdm(total=len(chunk_files), desc="Chunk Processing") as pbar, \
         Pool(min(cpu_count(), len(chunk_files))) as pool:
        for _ in pool.imap_unordered(process_chunk, [(c, results) for c in chunk_files]):
            pbar.update(1)

    # aggregate
    all_matches = []
    total_recs = total_matched = 0
    for data, recs, matched in results:
        all_matches.extend(data)
        total_recs   += recs
        total_matched += matched

    print(f"\nRecords scanned: {total_recs}, with emails: {total_matched}, total matches: {len(all_matches)}")

    merge_word()
    write_excel(all_matches)

    shutil.rmtree(TEMP_DIR)
    print(f"\n→ Word saved to {OUTPUT_DOCX}")
    print(f"→ Excel saved to {OUTPUT_XLSX}")
